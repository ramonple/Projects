{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the information value for both numerical and categorical variables & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_iv(data, target_variable, top_n = 10):\n",
    "    # Separate numerical and categorical variables\n",
    "    numerical_vars = data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_vars = data.select_dtypes(include=[np.object, np.category]).columns\n",
    "\n",
    "    # Impute missing values for numerical variables with -1\n",
    "    data[numerical_vars] = data[numerical_vars].fillna(-1)\n",
    "\n",
    "    # Impute missing values for categorical variables with 'missing'\n",
    "    data[categorical_vars] = data[categorical_vars].fillna('missing')\n",
    "\n",
    "    iv_results = []\n",
    "\n",
    "    # Calculate IV for numerical variables\n",
    "    for num_var in numerical_vars:\n",
    "        # Bin numerical variable\n",
    "        bins = pd.cut(data[num_var], bins=10, duplicates='drop')\n",
    "\n",
    "        # Create a new DataFrame with binned variable and target variable\n",
    "        binned_data = pd.DataFrame({num_var: bins, target_variable: data[target_variable]})\n",
    "\n",
    "        # Calculate percentage of events and non-events in each bin\n",
    "        pivot_table = binned_data.pivot_table(index=num_var, columns=target_variable, aggfunc='size', fill_value=0)\n",
    "        pivot_table['total'] = pivot_table.sum(axis=1)\n",
    "        pivot_table['event_rate'] = pivot_table[1] / pivot_table['total']\n",
    "        pivot_table['non_event_rate'] = pivot_table[0] / pivot_table['total']\n",
    "\n",
    "        # Calculate Weight of Evidence (WoE) and Information Value (IV)\n",
    "        pivot_table['woe'] = np.log(pivot_table['non_event_rate'] / pivot_table['event_rate'])\n",
    "        pivot_table['iv'] = (pivot_table['non_event_rate'] - pivot_table['event_rate']) * pivot_table['woe']\n",
    "        iv = pivot_table['iv'].sum()\n",
    "\n",
    "        iv_results.append((num_var, iv))\n",
    "\n",
    "    # Calculate IV for categorical variables\n",
    "    for cat_var in categorical_vars:\n",
    "        # Create a pivot table for the categorical variable and the target variable\n",
    "        pivot_table = data.pivot_table(index=cat_var, columns=target_variable, aggfunc='size', fill_value=0)\n",
    "\n",
    "        # Calculate percentage of events and non-events in each category\n",
    "        pivot_table['total'] = pivot_table.sum(axis=1)\n",
    "        pivot_table['event_rate'] = pivot_table[1] / pivot_table['total']\n",
    "        pivot_table['non_event_rate'] = pivot_table[0] / pivot_table['total']\n",
    "\n",
    "        # Calculate Weight of Evidence (WoE) and Information Value (IV)\n",
    "        pivot_table['woe'] = np.log(pivot_table['non_event_rate'] / pivot_table['event_rate'])\n",
    "        pivot_table['iv'] = (pivot_table['non_event_rate'] - pivot_table['event_rate']) * pivot_table['woe']\n",
    "        iv = pivot_table['iv'].sum()\n",
    "\n",
    "        iv_results.append((cat_var, iv))\n",
    "\n",
    "    # Sort IV results in descending order of IV value\n",
    "    iv_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Plot top 10 features\n",
    "    top_features = [x[0] for x in iv_results[:top_n]]\n",
    "    top_iv = [x[1] for x in iv_results[:top_n]]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_features, top_iv, color='skyblue')\n",
    "    plt.xlabel('Information Value')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top {top_n} Features by Information Value')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to show highest IV at the top\n",
    "    plt.show()\n",
    "\n",
    "    return iv_results\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'data' is your DataFrame and 'target_variable' is the dependent variable\n",
    "# calculate_iv(data, target_variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the feature importance of classifier /coefficients of regressor & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_features(model, feature_names, top_n=10):\n",
    "    # Get feature importances\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        feature_importances = np.abs(model.coef_)\n",
    "    else:\n",
    "        raise ValueError(\"Model does not support feature importances or coefficients\")\n",
    "\n",
    "    # Get indices of features sorted by importance\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    # Select top features\n",
    "    top_indices = sorted_indices[:top_n]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    top_importances = feature_importances[top_indices]\n",
    "\n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_features, top_importances, color='skyblue')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top {top_n} Feature Importances')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to show highest importance at the top\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'model' is your trained machine learning model and 'feature_names' are the names of your features\n",
    "# plot_top_features(model, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML classifier Evaluation & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_classifier_performance(model, X_test, y_test):\n",
    "    # Predict probabilities\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label='PR AUC = %0.2f' % pr_auc)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='red', lw=2, label='ROC AUC = %0.2f' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion matrix\n",
    "    y_pred = model.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix (Volume)')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.xticks(np.arange(2) + 0.5, ['Predicted Class 0', 'Predicted Class 1'])\n",
    "    plt.yticks(np.arange(2) + 0.5, ['True Class 0', 'True Class 1'], rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "    conf_matrix_percent = conf_matrix / np.sum(conf_matrix)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix_percent, annot=True, fmt='.2%', cmap='Blues')\n",
    "    plt.title('Confusion Matrix (Volume %)')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.xticks(np.arange(2) + 0.5, ['Predicted Class 0', 'Predicted Class 1'])\n",
    "    plt.yticks(np.arange(2) + 0.5, ['True Class 0', 'True Class 1'], rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Metrics from confusion matrix\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    accuracy = (tp + tn) / np.sum(conf_matrix)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nSummary of Model Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'model' is your trained classifier model, and 'X_test', 'y_test' are your test data\n",
    "# evaluate_classifier_performance(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - plot by group Actual vs Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_performance(data, selected_variables, actual_value, predicted_value):\n",
    "    # Plot the data for each categorical variable\n",
    "    for cat in selected_variables:\n",
    "        # Group data by the current categorical variable and calculate means\n",
    "        grouped_data = data.groupby(cat).agg({\n",
    "            actual_value: 'mean',\n",
    "            predicted_value: 'mean',\n",
    "            'volume': 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "        # Calculate upper and lower bounds for actual value\n",
    "        upper_bound = data[actual_value] * 1.1\n",
    "        lower_bound = data[actual_value] * 0.9\n",
    "\n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot actual and predicted values\n",
    "        plt.plot(data[actual_value], label='Actual', color='blue')\n",
    "        plt.plot(data[predicted_value], label='Predicted', color='red')\n",
    "\n",
    "        # Fill between upper and lower bounds\n",
    "        plt.fill_between(np.arange(len(data)), upper_bound, lower_bound, color='blue', alpha=0.2)\n",
    "\n",
    "        # Plot the volume bar chart\n",
    "        grouped_data['volume'].plot(kind='bar', color='green', alpha=0.5, secondary_y=True)\n",
    "\n",
    "        # Set labels and legend\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title(f'Actual vs Predicted with Volume for {cat}')\n",
    "        plt.legend()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame containing the data\n",
    "# Assuming 'selected_variables' is a list of categorical variables to group by\n",
    "# Assuming 'y_test' and 'y_pred' are the names of your actual and predicted variables\n",
    "plot_performance(df, ['cat_variable'], 'y_test', 'y_pred')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Model Building + hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Define the LightGBM model\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': sp_randint(50, 200),  # Number of trees\n",
    "    'max_depth': sp_randint(3, 15),         # Maximum depth of trees\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],   # Learning rate\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # Subsample ratio of columns when constructing each tree\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],         # L1 regularization term on weights\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0]         # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# Perform Randomized Search Cross Validation\n",
    "random_search = RandomizedSearchCV(model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=100, cv=3, \n",
    "                                   scoring='accuracy', \n",
    "                                   random_state=42)\n",
    "                                   \n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters found by RandomizedSearchCV:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Fit the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Accuracy of the best model on the test set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# save the model\n",
    "joblib.dump(model, 'model.pkl')\n",
    "\n",
    "# load the previously saved model\n",
    "loaded_model = joblib.load('model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression & output the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "\n",
    "# Step 1: Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Choose a Model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Fine-Tune the Model (Optional)\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "    'penalty': ['l1', 'l2']  # Regularization penalty\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Get the coefficients of the model\n",
    "coefficients = best_model.coef_[0]\n",
    "\n",
    "# Get the intercept of the model\n",
    "intercept = best_model.intercept_[0]\n",
    "\n",
    "# Get the variable names\n",
    "variable_names = X_train.columns\n",
    "\n",
    "# Print the equation\n",
    "equation = \"Prediction = \"\n",
    "equation += f\"({intercept:.4f})\"  # Include the intercept term\n",
    "for i, (coef, var) in enumerate(zip(coefficients, variable_names)):\n",
    "    equation += f\"({coef:.4f} * {var})\"\n",
    "    if i < len(coefficients) - 1:\n",
    "        equation += \" + \"\n",
    "print(\"Logistic Regression Equation:\")\n",
    "print(equation)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
